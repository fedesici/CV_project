{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9def9e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from time import perf_counter\n",
    "import torch.nn.functional as F\n",
    "from fastdepth_model import MobileNetSkipAdd\n",
    "from dataset import NYUDepthDataset\n",
    "import time \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253bcfd8",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration dictionary\n",
    "CONFIG = {\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"TRAIN_DIR\": \"nyu_data/data/nyu2_train\",\n",
    "    \"TEST_DIR\": \"nyu_data/data/nyu2_test\",\n",
    "    \"LEARNING_RATE\": 1e-3, \n",
    "    \"BATCH_SIZE\": 16,\n",
    "    \"EPOCHS\": 20,\n",
    "    \"IMG_WIDTH\": 320,\n",
    "    \"IMG_HEIGHT\": 240,\n",
    "    \"MAX_DEPTH_METERS\": 10.0,\n",
    "    \"ENABLE_SELECTIVITY\": True, #change if you want to enable/disable selectivity training\n",
    "    \"SELECTIVITY_LAYER_NAME\": \"decoder.ups_block_3\",#choose the layer to apply selectivity\n",
    "    \"SELECTIVITY_LAMBDA\": 0.1,#weight of the selectivity loss\n",
    "    \"NUM_DEPTH_BINS\": 27,# number of depth bins for selectivity\n",
    "    \"RESUME_CHECKPOINT\": None,#checkpoint path to resume training, none to start from the beginning\n",
    "}\n",
    "if CONFIG[\"ENABLE_SELECTIVITY\"]:\n",
    "    CONFIG[\"MODEL_SUFFIX\"] = \"_interpretable\"\n",
    "else:\n",
    "    CONFIG[\"MODEL_SUFFIX\"] = \"_baseline\" \n",
    "\n",
    "CONFIG[\"BEST_MODEL_PATH\"] = f\"models/best_model_meta{CONFIG['MODEL_SUFFIX']}.pth\"\n",
    "CONFIG[\"LATEST_CHECKPOINT_PATH\"] = f\"checkpoint/latest_checkpoint{CONFIG['MODEL_SUFFIX']}.pth\"\n",
    "CONFIG[\"RESUME_CHECKPOINT\"] = None\n",
    "\n",
    "print(f\"Device in use: {CONFIG['DEVICE']}\")\n",
    "print(f\"Training with selectivity: {'Enabled' if CONFIG['ENABLE_SELECTIVITY'] else 'Disabled'}\")\n",
    "if CONFIG['ENABLE_SELECTIVITY']:\n",
    "    print(f\"Layer Target: {CONFIG['SELECTIVITY_LAYER_NAME']}\")\n",
    "    print(f\"Lambda: {CONFIG['SELECTIVITY_LAMBDA']}\")\n",
    "    print(f\"Num Bin: {CONFIG['NUM_DEPTH_BINS']}\")\n",
    "print(f\"Best model to save in: {CONFIG['BEST_MODEL_PATH']}\")\n",
    "print(f\"Last checkpoint: {CONFIG['LATEST_CHECKPOINT_PATH']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7eb8ea",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2528ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function to discretize the depth range in a logarithmic way\n",
    "def discretize_depth(depth, num_bins, max_depth, min_depth=0.1): \n",
    "    mask = (depth > min_depth) & (depth < max_depth)#mask for valid values\n",
    "    bins = torch.full_like(depth, -1, dtype=torch.long)#initialize bins with -1\n",
    "    \n",
    "    depth_log = torch.log(depth[mask])# convert to log space\n",
    "    min_log = torch.log(torch.tensor(min_depth, device=depth.device))\n",
    "    max_log = torch.log(torch.tensor(max_depth, device=depth.device))\n",
    "    \n",
    "    normalized_depth = (depth_log - min_log) / (max_log - min_log)# normalize from 0 to 1\n",
    "    bin_values = torch.floor(normalized_depth * num_bins).long()# multiply by num bins and floor to get bin index\n",
    "    \n",
    "    bins[mask] = torch.clamp(bin_values, 0, num_bins - 1)# insert valid bins into the output tensor\n",
    "    return bins\n",
    "\n",
    "#function to capture the activations\n",
    "activations_capture = {} #dictionary to store the activations\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations_capture[name] = output #take the output of the layer\n",
    "    return hook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736f7c39",
   "metadata": {},
   "source": [
    "## Calculate valid bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = CONFIG['TRAIN_DIR'] \n",
    "depth_files = sorted(glob.glob(os.path.join(train_dir, '**', '*.png'), recursive=True))\n",
    "num_bins = CONFIG['NUM_DEPTH_BINS']\n",
    "max_depth = CONFIG['MAX_DEPTH_METERS']\n",
    "pixel_counts_np = np.zeros(num_bins, dtype=np.int64)# inizialize pixel counts array\n",
    "\n",
    "for depth_path in tqdm(depth_files, desc=\"Count pixels in bins\"):\n",
    "    with Image.open(depth_path) as depth_pil:# open the depth map\n",
    "        depth_array_raw = np.array(depth_pil)# convert the depth map to numpy array\n",
    "        \n",
    "        if depth_array_raw.max() <= 255:#handle 8 and 16 bit images\n",
    "            depth_meters = depth_array_raw.astype(np.float32) / 255.0 * max_depth\n",
    "        else:\n",
    "            depth_meters = depth_array_raw.astype(np.float32) / 1000.0\n",
    "        \n",
    "        depth_tensor = torch.from_numpy(depth_meters) # convert to tensor\n",
    "        gt_bins = discretize_depth(depth_tensor, num_bins, max_depth).numpy() #discretize the depth map\n",
    "        \n",
    "        valid_bins = gt_bins[gt_bins != -1]#remove invalid bins\n",
    "        \n",
    "        if valid_bins.size > 0: #if there are valid bins\n",
    "            bin_counts_batch = np.bincount(valid_bins, minlength=num_bins)# count pixels per bin\n",
    "            pixel_counts_np += bin_counts_batch# update global pixel counts\n",
    "\n",
    "pixel_counts = torch.from_numpy(pixel_counts_np)# convert to tensor\n",
    "valid_bin_ids = torch.where(pixel_counts > 0)[0].to(CONFIG['DEVICE'])#get indices of valid bins\n",
    "print(f\"\\nValid bins: {valid_bin_ids.tolist()}\")\n",
    "if len(valid_bin_ids) == 0:\n",
    "    raise ValueError(\"No depth bins found\")\n",
    "\n",
    "#function to assign depth to units\n",
    "def assign_depths_to_units(num_units, valid_bins):\n",
    "    num_valid_bins = len(valid_bins)#count valid bins\n",
    "    if num_valid_bins == 0:\n",
    "        return torch.full((num_units,), -1, dtype=torch.long, device=valid_bins.device)\n",
    "    assign_ids = torch.floor( torch.arange(num_units, device=valid_bins.device) * (num_valid_bins / num_units)).long()# genrate assignment indices\n",
    "    assign_ids = torch.clamp(assign_ids, 0, num_valid_bins - 1) #clamp to valid range\n",
    "    return valid_bins[assign_ids] #map the assigned indices to valid bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e69e063",
   "metadata": {},
   "source": [
    "## Define Selectivity Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectivityLoss(nn.Module):\n",
    "    def __init__(self, num_units, num_bins, valid_bins):\n",
    "        super().__init__()\n",
    "        self.num_units = num_units #num of units in the layer\n",
    "        self.num_bins_config = num_bins \n",
    "        \n",
    "        assigned_depths = assign_depths_to_units(num_units, valid_bins)#assign depths to units\n",
    "        self.register_buffer('assigned_depths', assigned_depths)# register as buffer\n",
    "\n",
    "    def forward(self, activations, gt_depths, max_depth, min_depth=0.1):\n",
    "        device = activations.device\n",
    "        batch_size, num_units, _, _ = activations.shape #get the batch size and num of units of the layer\n",
    "\n",
    "        gt_bins = discretize_depth(gt_depths, self.num_bins_config, max_depth, min_depth)#discretize ground truth depths\n",
    "        \n",
    "        if activations.shape[-2:] != gt_depths.shape[-2:]: #resize depth activations if needed\n",
    "            activations_resized = F.interpolate(activations, size=gt_depths.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        else:\n",
    "            activations_resized = activations\n",
    "\n",
    "        abs_activations = torch.abs(activations_resized)# take absolute value of activations\n",
    "        \n",
    "        avg_responses_batch = torch.zeros(batch_size, num_units, self.num_bins_config, device=device) #inizialize a tensor to store average responses\n",
    "        \n",
    "        for d in range(self.num_bins_config):\n",
    "            mask = (gt_bins == d).float() #mask for current bin\n",
    "            pixel_counts = torch.sum(mask, dim=[1, 2, 3]) + 1e-8 #count valid pixels per image\n",
    "            sum_activations = torch.sum(abs_activations * mask, dim=[2, 3])# sum activations for the current bin\n",
    "            avg_responses_batch[:, :, d] = sum_activations / pixel_counts.unsqueeze(1)# insert average responses for the current bin\n",
    "\n",
    "        total_loss = 0.0\n",
    "        data_units = 0\n",
    "\n",
    "        new_num_bins = avg_responses_batch.shape[2]# get the actual number of bins in the tensor\n",
    "\n",
    "        for i in range(self.num_units):\n",
    "            bin_i = self.assigned_depths[i].item()#get the assigned bin for the current unit\n",
    "            \n",
    "            if bin_i >= new_num_bins: #if the assigned bin is out of range\n",
    "                print(f\"Jump the unit {i} because its assigned bin ({bin_i}) is out of range \"\n",
    "                      f\"from the tensor with dimension: {new_num_bins}).\")\n",
    "                continue \n",
    "\n",
    "            r_bin_i = avg_responses_batch[:, i, bin_i] #extract the response for the assigned bin\n",
    "\n",
    "            other_bins_mask = torch.ones(new_num_bins, dtype=bool, device=device)# mask for other bins\n",
    "            other_bins_mask[bin_i] = False # exclude the assigned bin\n",
    "            \n",
    "            valid_bins_in_batch = (torch.sum(avg_responses_batch[:, i, other_bins_mask], dim=0) > 0)\n",
    "            if torch.any(valid_bins_in_batch):\n",
    "                r_mean = torch.mean(avg_responses_batch[:, i, other_bins_mask][:, valid_bins_in_batch], dim=1)#mean response for other bins\n",
    "            else:\n",
    "                r_mean = torch.zeros_like(r_bin_i)\n",
    "\n",
    "            num = r_bin_i - r_mean\n",
    "            den = r_bin_i + r_mean + 1e-8\n",
    "            ds_score = num / den #selectivity score\n",
    "            loss = -torch.mean(ds_score)#minimize selectivity score\n",
    "            \n",
    "            if not torch.isnan(loss):\n",
    "                total_loss += loss\n",
    "                data_units += 1\n",
    "\n",
    "        if data_units == 0:\n",
    "            return torch.tensor(0.0, device=device)\n",
    "\n",
    "        return total_loss / data_units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30fa56",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf75d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['IMG_HEIGHT'], CONFIG['IMG_WIDTH'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "depth_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['IMG_HEIGHT'], CONFIG['IMG_WIDTH']), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "])\n",
    "\n",
    "try:\n",
    "    #subset for train\n",
    "    train_dataset = NYUDepthDataset(\n",
    "        CONFIG['TRAIN_DIR'], \n",
    "        max_depth=CONFIG['MAX_DEPTH_METERS'],\n",
    "        transform=image_transform, \n",
    "        depth_transform=depth_transform, \n",
    "        train=True, \n",
    "        val=False  # Non è validation\n",
    "    )\n",
    "    \n",
    "    # subset for val\n",
    "    val_dataset = NYUDepthDataset(\n",
    "        CONFIG['TRAIN_DIR'], \n",
    "        max_depth=CONFIG['MAX_DEPTH_METERS'],\n",
    "        transform=image_transform, \n",
    "        depth_transform=depth_transform, \n",
    "        train=False, # false for validation\n",
    "        val=True     # true for iterate over training folder\n",
    "    )\n",
    "    \n",
    "    #test set\n",
    "    test_dataset = NYUDepthDataset(\n",
    "        CONFIG['TEST_DIR'], \n",
    "        max_depth=CONFIG['MAX_DEPTH_METERS'],\n",
    "        transform=image_transform, \n",
    "        depth_transform=depth_transform, \n",
    "        train=False, \n",
    "        val=False\n",
    "    )\n",
    "\n",
    "    assert len(train_dataset) == len(val_dataset) #check if train and val have the same lenght before split\n",
    "\n",
    "    #divide indices for train and val\n",
    "    val_percent = 0.1\n",
    "    n_total = len(train_dataset)\n",
    "    n_val = int(n_total * val_percent)\n",
    "    n_train = n_total - n_val\n",
    "    \n",
    "    generator = torch.Generator().manual_seed(19)\n",
    "    train_ids, val_ids = random_split(range(n_total), [n_train, n_val], generator=generator)\n",
    "\n",
    "    #create subsets\n",
    "    train_subset = Subset(train_dataset, train_ids)\n",
    "    val_subset = Subset(val_dataset, val_ids)\n",
    "\n",
    "    #create dataloaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=8)\n",
    "    val_loader = DataLoader(val_subset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=8)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=8)\n",
    "\n",
    "    print(f\"Dataset loaded successfully:\")\n",
    "    print(f\"Training set:   {len(train_subset)} samples \")\n",
    "    print(f\"Validation set: {len(val_subset)} samples \")\n",
    "    print(f\"Test set:       {len(test_dataset)} samples \")\n",
    "\n",
    "except (FileNotFoundError, AssertionError, TypeError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b4ec3",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d76386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#region: MobileViT Model Definition\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, depth=1, bias=False):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   groups=in_channels,\n",
    "                                   padding=1,\n",
    "                                   stride=stride,\n",
    "                                   bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1), bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        SeparableConv2d(in_channels=inp, out_channels=oup, kernel_size=kernal_size, stride=stride, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class ModLayerNorm(nn.GroupNorm):\n",
    "  def __init__(self, dim):\n",
    "      super().__init__(1, dim)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = ModLayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(dim,hidden_dim,1)\n",
    "        self.act = nn.ReLU()\n",
    "        self.c2 = nn.Conv2d(hidden_dim,dim,1)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, pool_size):\n",
    "      super().__init__()\n",
    "      self.pool = nn.AvgPool2d(pool_size, stride=1, padding=pool_size//2, count_include_pad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "      return self.pool(x) - x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(3)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class MV2Block(nn.Module):\n",
    "    def __init__(self, inp, oup, stride=1, expansion=4):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(inp * expansion)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expansion == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.ph, self.pw = patch_size\n",
    "\n",
    "        self.conv1 = conv_nxn_bn(channel, channel, kernel_size)\n",
    "        self.conv2 = conv_1x1_bn(channel, dim)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout)\n",
    "\n",
    "        self.conv3 = conv_1x1_bn(dim, channel)\n",
    "        self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x.clone()\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.cat((x, y), 1)\n",
    "        x = self.conv4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MobileViT(nn.Module):\n",
    "    def __init__(self, image_size, dims, channels, expansion=4, kernel_size=3, patch_size=(2, 2)):\n",
    "        super().__init__()\n",
    "        ih, iw = image_size\n",
    "        ph, pw = patch_size\n",
    "        assert ih % ph == 0 and iw % pw == 0\n",
    "\n",
    "        L = [1, 1, 1]\n",
    "\n",
    "        self.conv1 = conv_nxn_bn(3, channels[0], stride=2)\n",
    "\n",
    "        self.mv2 = nn.ModuleList([])\n",
    "        self.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
    "\n",
    "        self.mvit = nn.ModuleList([])\n",
    "        self.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0] * 2)))\n",
    "        self.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1] * 4)))\n",
    "        self.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2] * 4)))\n",
    "\n",
    "        self.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        y0 = self.conv1(x)\n",
    "        x = self.mv2[0](y0)\n",
    "        y1 = self.mv2[1](x)\n",
    "        x = self.mv2[2](y1)\n",
    "        x = self.mv2[3](x)\n",
    "        y2 = self.mv2[4](x)\n",
    "        x = self.mvit[0](y2)\n",
    "        y3 = self.mv2[5](x)\n",
    "        x = self.mvit[1](y3)\n",
    "        x = self.mv2[6](x)\n",
    "        x = self.mvit[2](x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x, [y0, y1, y2, y3]\n",
    "\n",
    "def mobilevit_xxs(transformer_times, sample_cnt): \n",
    "    enc_type = 'xxs'\n",
    "    dims = [64, 80, 96]\n",
    "    channels = [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 160]  # 320\n",
    "    return MobileViT((CONFIG[\"IMG_HEIGHT\"],  CONFIG[\"IMG_WIDTH\"]), dims, channels), enc_type \n",
    "\n",
    "\n",
    "def mobilevit_xs(transformer_times, sample_cnt):\n",
    "    enc_type = 'xs'\n",
    "    dims = [96, 120, 144]\n",
    "    channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 192] # 384\n",
    "    return MobileViT((CONFIG[\"IMG_HEIGHT\"], CONFIG[\"IMG_WIDTH\"]), dims, channels), enc_type \n",
    "\n",
    "\n",
    "def mobilevit_s(transformer_times, sample_cnt): \n",
    "    enc_type = 's'\n",
    "    dims = [144, 192, 240]\n",
    "    channels = [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 320]\n",
    "    return MobileViT((CONFIG[\"IMG_HEIGHT\"],  CONFIG[\"IMG_WIDTH\"]), dims, channels), enc_type \n",
    "\n",
    "class UpSample_layer(nn.Module):\n",
    "    def __init__(self, inp, oup, sep_conv_filters):\n",
    "        super(UpSample_layer, self).__init__()\n",
    "        self.conv2d_transpose = nn.ConvTranspose2d(inp, oup, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1),\n",
    "                                                   dilation=1, output_padding=(1, 1), bias=False)\n",
    "        self.end_up_layer = nn.Sequential(\n",
    "            SeparableConv2d(sep_conv_filters, oup, kernel_size=(3, 3)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, enc_layer):\n",
    "        x = self.conv2d_transpose(x)\n",
    "        if x.shape[2] != enc_layer.shape[2] or x.shape[3] != enc_layer.shape[3]:\n",
    "             x = F.interpolate(x, size=(enc_layer.shape[2], enc_layer.shape[3]), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        x = torch.cat([x, enc_layer], dim=1)\n",
    "        x = self.end_up_layer(x)\n",
    "        return x\n",
    "\n",
    "class SPEED_decoder(nn.Module):\n",
    "    def __init__(self, typ):\n",
    "        super(SPEED_decoder, self).__init__()\n",
    "        self.conv2d_in = nn.Conv2d(320 if typ == 's' else 192 if typ == 'xs' else 160,\n",
    "                                   128 if typ == 's' else 128 if typ == 'xs' else 64,\n",
    "                                   kernel_size=(1, 1), padding='same', bias=False)\n",
    "        self.ups_block_1 = UpSample_layer(128 if typ == 's' else 128 if typ == 'xs' else 64,\n",
    "                                          64 if typ == 's' else 64 if typ == 'xs' else 32,\n",
    "                                          sep_conv_filters=192 if typ == 's' else 144 if typ == 'xs' else 96)\n",
    "        self.ups_block_2 = UpSample_layer(64 if typ == 's' else 64 if typ == 'xs' else 32,\n",
    "                                          32 if typ == 's' else 32 if typ == 'xs' else 16,\n",
    "                                          sep_conv_filters=128 if typ == 's' else 96 if typ == 'xs' else 64)\n",
    "        self.ups_block_3 = UpSample_layer(32 if typ == 's' else 32 if typ == 'xs' else 16,\n",
    "                                          16 if typ == 's' else 16 if typ == 'xs' else 8,\n",
    "                                          sep_conv_filters=80 if typ == 's' else 64 if typ == 'xs' else 32)\n",
    "        self.final_upsample = nn.ConvTranspose2d(16 if typ == 's' else 16 if typ == 'xs' else 8, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
    "        \n",
    "        self.conv2d_out = nn.Conv2d(1, 1, kernel_size=(3, 3), padding='same', bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x, enc_layer_list):\n",
    "        x = self.conv2d_in(x)\n",
    "        x = self.ups_block_1(x, enc_layer_list[3])\n",
    "        x = self.ups_block_2(x, enc_layer_list[2])\n",
    "        x = self.ups_block_3(x, enc_layer_list[1])\n",
    "        x = self.final_upsample(x)\n",
    "        x = self.conv2d_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class build_model(nn.Module):\n",
    "   \n",
    "    def __init__(self, arch_type):\n",
    "        super(build_model, self).__init__()\n",
    "        self.transformer_times = np.zeros((3,655),dtype='float') \n",
    "        self.sample_cnt = 0 \n",
    "\n",
    "        if arch_type == 's':\n",
    "            self.encoder, enc_type = mobilevit_s(self.transformer_times, self.sample_cnt) \n",
    "        elif arch_type == 'xs':\n",
    "            self.encoder, enc_type = mobilevit_xs(self.transformer_times, self.sample_cnt) \n",
    "        else:\n",
    "            self.encoder, enc_type = mobilevit_xxs(self.transformer_times, self.sample_cnt)\n",
    "        self.decoder = SPEED_decoder( typ=enc_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_enc, enc_layers = self.encoder(x)\n",
    "        x_dec = self.decoder(x_enc, enc_layers)\n",
    "        return x_dec\n",
    "#endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old model\n",
    "# model = MobileNetSkipAdd(output_size=(224, 224), pretrained=True)\n",
    "# model.to(CONFIG['DEVICE'])\n",
    "\n",
    "\n",
    "ARCH_TYPE = 's'\n",
    "model = build_model(arch_type=ARCH_TYPE)\n",
    "model.to(CONFIG['DEVICE'])\n",
    "\n",
    "\n",
    "print(\"Models names:\")\n",
    "for name, _ in model.named_modules():\n",
    "    print(name)\n",
    "\n",
    "print(\"\\nModels architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dec7cd3",
   "metadata": {},
   "source": [
    "## Assign Depths to Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    target_layer = dict(model.named_modules())[CONFIG['SELECTIVITY_LAYER_NAME']]# get the target layer\n",
    "    \n",
    "    conv_layer = [m for m in target_layer.modules() if isinstance(m, nn.Conv2d)]# find all Conv2d layers in the target module\n",
    "    if not conv_layer:\n",
    "        raise ValueError(\"No layer found\")\n",
    "    layer_units = conv_layer[-1].out_channels#get the output channels of the last Conv2d layer\n",
    "    \n",
    "    assign_depths = assign_depths_to_units(layer_units, valid_bin_ids) #assign depths to units\n",
    "    \n",
    "    print(f\"Assigned {layer_units} units at {len(valid_bin_ids)} valid bins\")\n",
    " \n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nNo bins assigned: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f39414",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fac151",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, interpretable_loss_fn, curr_lambda, config):\n",
    "    model.train()\n",
    "    total_l1_loss = 0.0\n",
    "    total_sel_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training Epoch\")\n",
    "    for images, depths in pbar:\n",
    "        images, depths = images.to(config['DEVICE']), depths.to(config['DEVICE'])\n",
    "        optimizer.zero_grad() #zero the gradients of the previous step\n",
    "        \n",
    "        pred_depths = model(images) #forward pass\n",
    "        \n",
    "        predicted_depths_resized = F.interpolate(pred_depths, size=depths.shape[-2:], mode='bilinear', align_corners=False)# resize to match gt size\n",
    "        mask = depths > 0 #mask for valid depths\n",
    "        l1_loss = loss_fn(predicted_depths_resized[mask], depths[mask])# calculate L1 loss\n",
    "\n",
    "        sel_loss = torch.tensor(0.0, device=config['DEVICE'])\n",
    "        if curr_lambda > 0 and interpretable_loss_fn is not None:# if selectivity is enabled\n",
    "            layer_name = config['SELECTIVITY_LAYER_NAME']# get the target layer name\n",
    "            if layer_name in activations_capture:\n",
    "                interpretable_activations = activations_capture[layer_name]#take the activations from the hook\n",
    "                base_sel_loss = interpretable_loss_fn(interpretable_activations, depths, config['MAX_DEPTH_METERS'])\n",
    "                sel_loss = curr_lambda * base_sel_loss# weight the selectivity loss\n",
    "        \n",
    "        total_loss = l1_loss + sel_loss\n",
    "        if not torch.isnan(total_loss):\n",
    "            total_loss.backward()# backpropagation\n",
    "            optimizer.step()# update weights\n",
    "            total_l1_loss += l1_loss.item()\n",
    "            total_sel_loss += sel_loss.item()\n",
    "\n",
    "        pbar.set_postfix(l1_loss=f\"{l1_loss.item():.4f}\", sel_loss=f\"{sel_loss.item():.4f}\")\n",
    "    \n",
    "    return total_l1_loss / len(dataloader), total_sel_loss / len(dataloader)\n",
    "\n",
    "def validate_one_epoch(model, dataloader, loss_fn, device, desc=\"Validating\"):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, depths in tqdm(dataloader, desc=desc):\n",
    "            images, depths = images.to(device), depths.to(device)\n",
    "            predicted_depths = model(images)\n",
    "            predicted_depths = F.interpolate(predicted_depths, size=depths.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            mask = depths > 0\n",
    "            loss = loss_fn(predicted_depths[mask], depths[mask])# calculate L1 loss on valid set\n",
    "            if not torch.isnan(loss):\n",
    "                total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['LEARNING_RATE'])\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "interpretable_loss_fn = None\n",
    "hook_handle = None\n",
    "\n",
    "if CONFIG['ENABLE_SELECTIVITY']:\n",
    "    try:\n",
    "        target_layer_module = dict(model.named_modules())[CONFIG['SELECTIVITY_LAYER_NAME']]\n",
    "        \n",
    "        conv_layers = [m for m in target_layer_module.modules() if isinstance(m, nn.Conv2d)]\n",
    "        if not conv_layers:\n",
    "            raise ValueError(f\"No Conv2d layers found in the target module '{CONFIG['SELECTIVITY_LAYER_NAME']}'\")\n",
    "        num_units = conv_layers[-1].out_channels #take the output channels of the last layer\n",
    "        \n",
    "        interpretable_loss_fn = SelectivityLoss(# create the loss instance\n",
    "            num_units=num_units,\n",
    "            num_bins=CONFIG['NUM_DEPTH_BINS'],\n",
    "            valid_bins=valid_bin_ids\n",
    "        ).to(CONFIG['DEVICE'])\n",
    "        \n",
    "        target_layer = dict(model.named_modules())[CONFIG['SELECTIVITY_LAYER_NAME']]\n",
    "        hook_handle = target_layer.register_forward_hook(get_activation(CONFIG['SELECTIVITY_LAYER_NAME']))#take the activation with a hook and save into the dictionary activation capture\n",
    "\n",
    "    except (KeyError, AttributeError, NameError) as e:\n",
    "        print(f\"Error in the setup of the loss: {e}\")\n",
    "        raise\n",
    "\n",
    "start_epoch = 0\n",
    "best_val_loss = float('inf')\n",
    "if CONFIG.get('RESUME_CHECKPOINT') and os.path.exists(CONFIG['RESUME_CHECKPOINT']):#if wantt to resume a training\n",
    "    print(f\"Resume training from {CONFIG['RESUME_CHECKPOINT']}\")\n",
    "    checkpoint = torch.load(CONFIG['RESUME_CHECKPOINT'], map_location=CONFIG['DEVICE'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    print(f\"Checkpoint loaded {start_epoch}, best_val_loss = {best_val_loss:.4f}\")\n",
    "else:\n",
    "    print(\"Start a new training\")\n",
    "\n",
    "for epoch in range(start_epoch, CONFIG['EPOCHS']):#loop over epochs\n",
    "    print(f\"\\n--- Epoch [{epoch+1}/{CONFIG['EPOCHS']}] ---\")\n",
    "    \n",
    "    current_lambda = 0.0\n",
    "    if CONFIG['ENABLE_SELECTIVITY']:\n",
    "        current_lambda = CONFIG['SELECTIVITY_LAMBDA']\n",
    "\n",
    "    avg_l1_loss, avg_sel_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, interpretable_loss_fn, current_lambda, CONFIG)\n",
    "    \n",
    "    avg_val_loss = validate_one_epoch(model, val_loader, loss_fn, CONFIG['DEVICE'])\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{CONFIG['EPOCHS']}], Avg L1 Loss: {avg_l1_loss:.4f}, Avg Sel Loss: {avg_sel_loss:.4f}, Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save({'model_state_dict': model.state_dict()}, CONFIG['BEST_MODEL_PATH'])# save best model based on val loss\n",
    "        print(f\"New best model saved in '{CONFIG['BEST_MODEL_PATH']}' (Val Loss: {best_val_loss:.4f})\")\n",
    "        \n",
    "    latest_checkpoint = {#save latest checkpoint\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_val_loss': best_val_loss\n",
    "    }\n",
    "    torch.save(latest_checkpoint, CONFIG['LATEST_CHECKPOINT_PATH'])\n",
    "\n",
    "if hook_handle:\n",
    "    hook_handle.remove()# remove the hook\n",
    "\n",
    "print(\"\\nTraining completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f274628",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758c093",
   "metadata": {},
   "source": [
    "## Load best model or checkpoint path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint = torch.load(CONFIG['LATEST_CHECKPOINT_PATH'], map_location=CONFIG['DEVICE'])\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "checkpoint = torch.load(CONFIG['LATEST_CHECKPOINT_PATH'], map_location=CONFIG['DEVICE'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(CONFIG['DEVICE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fd8ccd",
   "metadata": {},
   "source": [
    "## Evaluate depth performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff30c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to evaluate depth performance metrics\n",
    "def evaluate_performance(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds, gts = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, gt_depths in tqdm(dataloader, desc=\"Evaluating Performance\"):\n",
    "            images, gt_depths_gpu = images.to(device), gt_depths.to(device)\n",
    "            pred_depths = model(images)#obtain predictions\n",
    "            pred_depths = F.interpolate(pred_depths, size=gt_depths_gpu.shape[-2:], mode='bilinear', align_corners=False)#resize\n",
    "            \n",
    "            mask = gt_depths_gpu > 0#mask for valid depths\n",
    "            valid_preds = pred_depths[mask].cpu()\n",
    "            valid_gts = gt_depths_gpu[mask].cpu()\n",
    "            preds.append(valid_preds)\n",
    "            gts.append(valid_gts)\n",
    "\n",
    "    preds = torch.cat(preds).numpy()\n",
    "    gts = torch.cat(gts).numpy()\n",
    "    \n",
    "    #calculate the metrics\n",
    "    abs_diff = np.abs(gts - preds)\n",
    "    mae = np.mean(abs_diff)\n",
    "    rmse = np.sqrt(np.mean((gts - preds) ** 2))\n",
    "    abs_rel = np.mean(abs_diff / gts)\n",
    "    \n",
    "    #calculate delta metrics\n",
    "    ratio = np.maximum((gts / preds), (preds / gts))\n",
    "    delta1 = (ratio < 1.25).mean()\n",
    "    delta2 = (ratio < 1.25**2).mean()\n",
    "    delta3 = (ratio < 1.25**3).mean()\n",
    "    \n",
    "    results = {\n",
    "        \"rmse\": rmse, \"mae\": mae, \"rel\": abs_rel,\n",
    "        \"delta1\": delta1, \"delta2\": delta2, \"delta3\": delta3\n",
    "    }\n",
    "    return results\n",
    "\n",
    "print(f\"\\nModel loaded:'{CONFIG['BEST_MODEL_PATH']}'\")\n",
    "\n",
    "\n",
    "performance_metrics = evaluate_performance(model, test_loader, CONFIG['DEVICE'])#call the evaluation function\n",
    "\n",
    "\n",
    "print(\"\\n Performance on Test Set:\")\n",
    "print(f\"RMSE:{performance_metrics['rmse']:.4f}\")\n",
    "print(f\"MAE: {performance_metrics['mae']:.4f}\")\n",
    "print(f\"REL: {performance_metrics['rel']:.4f}\")\n",
    "\n",
    "print(f\"δ < 1.25: {performance_metrics['delta1']:.4%}\")\n",
    "print(f\"δ < 1.25²: {performance_metrics['delta2']:.4%}\")\n",
    "print(f\"δ < 1.25³: {performance_metrics['delta3']:.4%}\")\n",
    "\n",
    "# function to visulize depth predictions\n",
    "def visualize_predictions(model, dataloader, device, num_samples=5):\n",
    "    model.eval()\n",
    "    samples_shown = 0\n",
    "    with torch.no_grad():\n",
    "        for images, gt_depths in dataloader:\n",
    "            images = images.to(device)\n",
    "            predicted_depths = model(images).cpu()\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                if samples_shown >= num_samples: return\n",
    "\n",
    "                img = images[i].cpu().permute(1, 2, 0).numpy()#convert the dimension for matplotlib\n",
    "                mean, std = np.array([0.485, 0.456, 0.406]), np.array([0.229, 0.224, 0.225])# denormalize\n",
    "                img = np.clip(std * img + mean, 0, 1)\n",
    "\n",
    "                gt = gt_depths[i].squeeze().numpy()#convert depth and prediction to numpy\n",
    "                pred = predicted_depths[i].squeeze().numpy()\n",
    "                \n",
    "                #plot the images gt and prediction\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "                axes[0].imshow(img); axes[0].set_title(\"RGB image\"); axes[0].axis('off')\n",
    "                axes[1].imshow(gt, cmap='magma', vmin=0, vmax=CONFIG['MAX_DEPTH_METERS']); axes[1].set_title(\"Ground Truth\"); axes[1].axis('off')\n",
    "                im = axes[2].imshow(pred, cmap='magma', vmin=0, vmax=CONFIG['MAX_DEPTH_METERS']); axes[2].set_title(\"Prediction\"); axes[2].axis('off')\n",
    "                fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.7)\n",
    "                plt.show()\n",
    "                samples_shown += 1\n",
    "\n",
    "visualize_predictions(model, test_loader, CONFIG['DEVICE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713f559",
   "metadata": {},
   "source": [
    "## Selectivity evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc930f8e",
   "metadata": {},
   "source": [
    "### Neuron selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03488d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to visualzie the selectivity of the neuron\n",
    "\n",
    "def visualize_neuron_selectivity(model, dataloader, layer_name, config, valid_bin_ids, num_units_to_show=32, num_batches=50):\n",
    "   \n",
    "    model.eval()\n",
    "    device = config['DEVICE']\n",
    "    num_bins = config['NUM_DEPTH_BINS']\n",
    "    max_depth = config['MAX_DEPTH_METERS']\n",
    "    \n",
    "    hook_handle = None\n",
    "    avg_responses = None \n",
    "    num_units = 0\n",
    "\n",
    "    try:\n",
    "        target_layer = dict(model.named_modules())[layer_name]\n",
    "        num_units = [m.out_channels for m in target_layer.modules() if isinstance(m, nn.Conv2d)][-1]\n",
    "        hook_handle = target_layer.register_forward_hook(get_activation(layer_name))\n",
    "        \n",
    "        sum_responses = torch.zeros(num_units, num_bins, device=device)\n",
    "        pixel_counts = torch.zeros(num_units, num_bins, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(total=min(num_batches, len(dataloader)), desc=\"Calculating Selectivity\")\n",
    "            for i, (images, gt_depths) in enumerate(dataloader):\n",
    "                if i >= num_batches: break\n",
    "                images, gt_depths = images.to(device), gt_depths.to(device)\n",
    "                _ = model(images)# forward pass to capture activations\n",
    "                \n",
    "                activations = activations_capture[layer_name]# get the activations from the hook\n",
    "                activations_resized = F.interpolate(activations, size=gt_depths.shape[-2:], mode='bilinear', align_corners=False)\n",
    "                gt_bins = discretize_depth(gt_depths, num_bins, max_depth)\n",
    "                abs_activations = torch.abs(activations_resized)# take absolute value of the activations\n",
    "                \n",
    "                for k in range(num_units):\n",
    "                    for d in range(num_bins):\n",
    "                        mask_d = (gt_bins == d).float()#mask for the current bin\n",
    "                        sum_responses[k, d] += torch.sum(abs_activations[:, k, :, :] * mask_d)#sum the activations for the current bin\n",
    "                        pixel_counts[k, d] += torch.sum(mask_d)# count valid pixels for the current bin\n",
    "                pbar.update(1)\n",
    "            pbar.close()\n",
    "\n",
    "        avg_responses = (sum_responses / (pixel_counts + 1e-8)).cpu().numpy()# calculate average responses\n",
    "\n",
    "    except (KeyError, AttributeError, IndexError) as e:\n",
    "        print(f\"error in visualization: {e}\")\n",
    "        return \n",
    "\n",
    "    finally:\n",
    "        if hook_handle:\n",
    "            hook_handle.remove()\n",
    "\n",
    "    if avg_responses is None:\n",
    "        return\n",
    "\n",
    "    indices_to_show = np.linspace(0, num_units - 1, min(num_units_to_show, num_units), dtype=int)#select units to show\n",
    "    assigned_depths_map = None\n",
    "    if config.get(\"ENABLE_SELECTIVITY\", False):\n",
    "        assigned_depths_tensor = assign_depths_to_units(num_units, valid_bin_ids.cpu())#assign target bin\n",
    "        assigned_depths_map = assigned_depths_tensor.numpy()\n",
    "        \n",
    "    print(f\"Visualize the units: {indices_to_show}\")\n",
    "    \n",
    "\n",
    "    fig, axes = plt.subplots(1, len(indices_to_show), figsize=(5 * len(indices_to_show), 4), sharey=True)\n",
    "    fig.suptitle(f\"Avg response of the layer'{layer_name}'at every depth bin\", fontsize=16)\n",
    "    \n",
    "    #calculate the depth values\n",
    "    log_min = np.log(0.1) \n",
    "    log_max = np.log(max_depth)\n",
    "    log_bin_edges = np.linspace(log_min, log_max, num_bins + 1)\n",
    "    depth_bin_values = np.exp(log_bin_edges[:-1]) #use the beginning of each bin as representative value\n",
    "    x_positions = np.arange(num_bins) #x axe is the number or bin\n",
    "\n",
    "    for i, unit_idx in enumerate(indices_to_show):\n",
    "        ax = axes if len(indices_to_show) == 1 else axes[i]#iterate over the units\n",
    "        responses = avg_responses[unit_idx]\n",
    "        ax.bar(x_positions, responses, width=0.8) #height of the bar is the average response\n",
    "        \n",
    "        title = f\"Unit {unit_idx}\"\n",
    "        if assigned_depths_map is not None:# if selectivity is enabled\n",
    "            assigned_bin = assigned_depths_map[unit_idx]#obtain the assigned bin\n",
    "            assigned_depth_val = depth_bin_values[assigned_bin]\n",
    "            title += f\"\\n(Target Bin: {assigned_bin} ≈ {assigned_depth_val:.1f}m)\"\n",
    "            if assigned_bin < len(ax.patches):\n",
    "                 ax.patches[assigned_bin].set_facecolor('orangered')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Depth (m)\")\n",
    "\n",
    "        num_labels = 5  \n",
    "        tick_indices = np.linspace(0, num_bins - 1, num_labels, dtype=int)#linear ticks\n",
    "        tick_labels = [f\"{depth_bin_values[idx]:.1f}\" for idx in tick_indices]\n",
    "        ax.set_xticks(tick_indices)\n",
    "        ax.set_xticklabels(tick_labels)\n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(\"Avg response\")\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_neuron_selectivity(model, train_loader, CONFIG['SELECTIVITY_LAYER_NAME'], CONFIG, valid_bin_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e91565",
   "metadata": {},
   "source": [
    "### Visualize activation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df97da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function to show activation maps\n",
    "def visualize_activation_maps(model, dataloader, layer_name, config, valid_bin_ids, num_images=3, image_idx=None, units_to_show=[1, 7, 13]):\n",
    "\n",
    "    model.eval()\n",
    "    device = config['DEVICE']\n",
    "    hook_handle = None\n",
    "\n",
    "    try:\n",
    "        target_layer = dict(model.named_modules())[layer_name]\n",
    "        hook_handle = target_layer.register_forward_hook(get_activation(layer_name))\n",
    "        num_units = [m.out_channels for m in target_layer.modules() if isinstance(m, nn.Conv2d)][-1]\n",
    "        \n",
    "        assigned_targets = assign_depths_to_units(num_units, valid_bin_ids.cpu())\n",
    "        log_min = np.log(0.1)\n",
    "        log_max = np.log(config['MAX_DEPTH_METERS'])\n",
    "        log_bin_edges = np.linspace(log_min, log_max, config['NUM_DEPTH_BINS'] + 1)\n",
    "        log_bin_centers = (log_bin_edges[:-1] + log_bin_edges[1:]) / 2\n",
    "        depth_bin_centers = np.exp(log_bin_centers)\n",
    "\n",
    "        print(f\"Visualize units: {units_to_show}\")\n",
    "        \n",
    "        images_shown = 0\n",
    "        curr_idx = 0\n",
    "        with torch.no_grad():\n",
    "            for images, gt_depths in dataloader:#iterate over batches\n",
    "                for i in range(images.size(0)):\n",
    "                    show = False\n",
    "                    if image_idx is not None:\n",
    "                        if curr_idx == image_idx:\n",
    "                            show = True\n",
    "                    elif images_shown < num_images:\n",
    "                        show = True\n",
    "\n",
    "                    if show:\n",
    "                        print(f\"\\nVisualize image number {curr_idx}\")\n",
    "                        \n",
    "                        image_tensor = images[i].unsqueeze(0).to(device)\n",
    "                        \n",
    "                        _ = model(image_tensor)\n",
    "                        activations = activations_capture[layer_name]\n",
    "\n",
    "                        #plot the image gt depth and feature maps\n",
    "                        img_rgb_normalized = images[i].permute(1, 2, 0).numpy()\n",
    "                        mean, std = np.array([0.485, 0.456, 0.406]), np.array([0.229, 0.224, 0.225])\n",
    "                        img_rgb = np.clip(std * img_rgb_normalized + mean, 0, 1)\n",
    "                        \n",
    "                        gt_depth = gt_depths[i].squeeze().numpy()\n",
    "                        \n",
    "                        num_plots = 2 + len(units_to_show)\n",
    "                        fig, axes = plt.subplots(1, num_plots, figsize=(6 * num_plots, 6))\n",
    "                        \n",
    "                        axes[0].imshow(img_rgb)\n",
    "                        axes[0].set_title(\"Image RGB\")\n",
    "                        axes[0].axis('off')\n",
    "\n",
    "                        axes[1].imshow(gt_depth, cmap='magma', vmin=0, vmax=config['MAX_DEPTH_METERS'])\n",
    "                        axes[1].set_title(\"Depth (m) \")\n",
    "                        axes[1].axis('off')\n",
    "                        \n",
    "                        for j, unit_idx in enumerate(units_to_show):\n",
    "                            activation_map = torch.abs(activations[0, unit_idx, :, :]).cpu().numpy()\n",
    "                            \n",
    "                            h, w, _ = img_rgb.shape\n",
    "                            activation_map_resized = cv2.resize(activation_map, (w, h))\n",
    "                            \n",
    "                            heatmap = cv2.normalize(activation_map_resized, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "                            heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "                            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "                            \n",
    "                            img_with_heatmap = cv2.addWeighted((img_rgb * 255).astype(np.uint8), 0.5, heatmap, 0.5, 0)\n",
    "                            \n",
    "                            ax = axes[2 + j]\n",
    "                            ax.imshow(img_with_heatmap)\n",
    "                            target_bin = assigned_targets[unit_idx].item()\n",
    "                            target_depth = depth_bin_centers[target_bin]\n",
    "                            ax.set_title(f\"Unit{unit_idx}\\n(Target: Bin {target_bin} ≈ {target_depth:.1f}m)\")\n",
    "                            ax.axis('off')\n",
    "\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                        \n",
    "                        images_shown += 1\n",
    "\n",
    "                    \n",
    "                    curr_idx += 1\n",
    "                    \n",
    "                    if (image_idx is not None and images_shown > 0) or \\\n",
    "                       (image_idx is None and images_shown >= num_images):\n",
    "                        break\n",
    "                \n",
    "                if (image_idx is not None and images_shown > 0) or \\\n",
    "                   (image_idx is None and images_shown >= num_images):\n",
    "                    break\n",
    "            \n",
    "    except (KeyError, IndexError, AttributeError) as e:\n",
    "        print(f\"error: {e}\")\n",
    "        return\n",
    "    finally:\n",
    "        if hook_handle:\n",
    "            hook_handle.remove()\n",
    "\n",
    "\n",
    "visualize_activation_maps(model, test_loader, CONFIG['SELECTIVITY_LAYER_NAME'], CONFIG, valid_bin_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1277e92b",
   "metadata": {},
   "source": [
    "### Evaluate selectivity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c994eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_selectivity(model, dataloader, layer_name, config, valid_bin_ids):\n",
    "    model.eval()\n",
    "    device = config['DEVICE']\n",
    "    num_bins = config['NUM_DEPTH_BINS']\n",
    "    max_depth = config['MAX_DEPTH_METERS']\n",
    "    \n",
    "    hook_handle = None\n",
    "    try:\n",
    "    \n",
    "        target_layer = dict(model.named_modules())[layer_name]\n",
    "        hook_handle = target_layer.register_forward_hook(get_activation(layer_name))\n",
    "        num_units = [m.out_channels for m in target_layer.modules() if isinstance(m, nn.Conv2d)][-1]\n",
    "        \n",
    "        sum_responses = torch.zeros(num_units, num_bins, device=device)\n",
    "        pixel_counts = torch.zeros(num_bins, device=device)\n",
    "\n",
    "        print(f\"Calculate selectivity for '{layer_name}' with ({num_units} units\")\n",
    "        with torch.no_grad():\n",
    "            for images, gt_depths in tqdm(dataloader, desc=\"Calculating Selectivity\"):\n",
    "                images, gt_depths = images.to(device), gt_depths.to(device)\n",
    "                _ = model(images) \n",
    "                \n",
    "                activations = activations_capture[layer_name]# get the activations from the hook\n",
    "                activations_resized = F.interpolate(activations, size=gt_depths.shape[-2:], mode='bilinear', align_corners=False)\n",
    "                gt_bins = discretize_depth(gt_depths, num_bins, max_depth)\n",
    "                abs_activations = torch.abs(activations_resized)\n",
    "                \n",
    "                for d in range(num_bins):\n",
    "                    mask_d = (gt_bins == d)#mask for the current bin\n",
    "                    sum_responses[:, d] += torch.sum(abs_activations * mask_d.float(), dim=(0, 2, 3))#sum activations for the current bin\n",
    "                    pixel_counts[d] += torch.sum(mask_d)# count valid pixels for the current bin\n",
    "        \n",
    "        avg_responses = sum_responses / (pixel_counts.unsqueeze(0) + 1e-8)\n",
    "    \n",
    "        assigned_targets = assign_depths_to_units(num_units, valid_bin_ids.to(device))# assign target bins to units\n",
    "        \n",
    "        ds_scores, ds_scores_target = [], []#list to store the ds scores\n",
    "        correct_assignments = 0# counter for correct assignments\n",
    "        count_non_active_units = 0\n",
    "        for k in range(num_units):\n",
    "            responses_k = avg_responses[k]\n",
    "            target_bin = assigned_targets[k].item()\n",
    "            if torch.all(responses_k == 0): \n",
    "                count_non_active_units += 1\n",
    "                continue#skip if no response in any bin\n",
    "            \n",
    "            R_max_val, R_max_idx = torch.max(responses_k), torch.argmax(responses_k)#find the max response and its index\n",
    "            other_mask = torch.ones(num_bins, dtype=bool, device=device); other_mask[R_max_idx] = False#mask for other bins\n",
    "            valid_bins_mask = pixel_counts > 0\n",
    "            final_mask = other_mask & valid_bins_mask\n",
    "            R_bar_other = torch.mean(responses_k[final_mask]) if torch.sum(final_mask) > 0 else 0.0#mean response for other bins\n",
    "            ds_generic = (R_max_val - R_bar_other) / (R_max_val + R_bar_other + 1e-8)#calcuculate the ds score\n",
    "            ds_scores.append(ds_generic.item())\n",
    "            \n",
    "            R_target_val = responses_k[target_bin]# response for the target bin\n",
    "            other_target_mask = torch.ones(num_bins, dtype=bool, device=device); other_target_mask[target_bin] = False#mean for other bins excluding the target\n",
    "            final_target_mask = other_target_mask & valid_bins_mask\n",
    "            R_bar_not_target = torch.mean(responses_k[final_target_mask]) if torch.sum(final_target_mask) > 0 else 0.0\n",
    "            ds_target = (R_target_val - R_bar_not_target) / (R_target_val + R_bar_not_target + 1e-8)\n",
    "            ds_scores_target.append(ds_target.item())\n",
    "            \n",
    "            if R_max_idx == target_bin:\n",
    "                correct_assignments += 1#coorect assignment if the max response bin is the target bin\n",
    "                \n",
    "        num_valid_units = len(ds_scores) if len(ds_scores) > 0 else 1\n",
    "        \n",
    "        results = {\n",
    "            \"avg_ds_score_generic\": np.mean(ds_scores) if ds_scores else 0.0,\n",
    "            \"avg_ds_score_target\": np.mean(ds_scores_target) if ds_scores_target else 0.0,\n",
    "            \"assignment_accuracy\": (correct_assignments / num_valid_units),\n",
    "            \"non_active_units\": count_non_active_units\n",
    "        }\n",
    "        \n",
    "\n",
    "        print(f\" DS Score: {results['avg_ds_score_generic']:.4f}\")\n",
    "        print(f\"Ds score target :{results['avg_ds_score_target']:.4f}\")\n",
    "        print(f\"Assigning accuracy {results['assignment_accuracy']:.2%}\")\n",
    "        print(f\"Non-active units: {results['non_active_units']} out of {num_units}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    except (KeyError, AttributeError, IndexError) as e:\n",
    "        print(f\"error {e}\")\n",
    "        return {\"avg_ds_score_generic\": 0.0, \"avg_ds_score_target\": 0.0, \"assignment_accuracy\": 0.0}\n",
    "\n",
    "    finally:\n",
    "        if hook_handle:\n",
    "            hook_handle.remove()\n",
    "\n",
    "selectivity_metrics = evaluate_selectivity(model, train_loader, CONFIG['SELECTIVITY_LAYER_NAME'], CONFIG, valid_bin_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e77b8",
   "metadata": {},
   "source": [
    "## Inference Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee8b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_device = 'cpu' #change here for the test\n",
    "model.to(inference_device)\n",
    "\n",
    "def inference_time(model, dataloader, device, num_warmup=10, num_test=41):\n",
    "    model.eval()\n",
    "    times = []\n",
    "\n",
    "    print(f\"Warmup\")\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(dataloader):\n",
    "            if i >= num_warmup: \n",
    "                break\n",
    "            images = images.to(device)\n",
    "            _ = model(images) #ignore the output\n",
    "\n",
    "    print(f\"Start test on {num_test} samples\")\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(total = num_test, desc=\"Measuring Inference Time\")\n",
    "        batches = 0\n",
    "        for images, _ in dataloader:\n",
    "            if batches >= num_test:\n",
    "                break\n",
    "            images = images.to(device)\n",
    "\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            _ = model(images)\n",
    "\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            end_time = time.perf_counter()\n",
    "\n",
    "            inference_time = end_time - start_time\n",
    "            times.append(inference_time / images.size(0))#time per image\n",
    "            batches += 1\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "        times_array = np.array(times)\n",
    "\n",
    "        avg_time_per_image = np.mean(times_array)\n",
    "\n",
    "\n",
    "        print(f\"Device: {device.upper()}\")\n",
    "        print(f\"Model: {CONFIG['MODEL_SUFFIX']}\")\n",
    "        print(f\"samples {batches* dataloader.batch_size}\")\n",
    "        print(f\"Avg time per image: {avg_time_per_image*1000:.2f} ms\")\n",
    "       \n",
    "inference_time(model, test_loader, inference_device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
